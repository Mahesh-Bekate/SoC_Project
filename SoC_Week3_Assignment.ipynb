{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0188193e",
   "metadata": {},
   "source": [
    "# The Image Cartoonifier SoCâ€™23\n",
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "16c70be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d683e6fc",
   "metadata": {},
   "source": [
    "The Digit Recognizer dataset consists of images of the numbers 0-9. Each image is represented\n",
    "as a 28x28 matrix, with each pixel having a single pixel-value associated with it. 784 pixels in\n",
    "each 28x28 input image.\n",
    "a) Split the data set into dev_set and train_set like X_dev, Y_dev for 1 to 1000 samples from the above data and X_train,Y_train from 1000 to m.\n",
    "b) Extract the pixel values except the label values and Normalize the pixel values by dividing\n",
    "them by 255 to bring them into the range of 0 to 1. Store the normalized pixel values in\n",
    "variables called X_dev,X_train respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4b1ffb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\")\n",
    "\n",
    "dev_set = df.iloc[:1000, :]\n",
    "train_set = df.iloc[1000: , :]\n",
    "\n",
    "X_dev = dev_set.iloc[:, 1:].values\n",
    "X_train = train_set.iloc[:, 1:].values\n",
    "\n",
    "Y_dev = dev_set.iloc[:, 0].values\n",
    "Y_train = train_set.iloc[:, 0].values\n",
    "\n",
    "X_dev = X_dev/255\n",
    "X_train = X_train/255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab9621d",
   "metadata": {},
   "source": [
    "## Part 1: Neural Network Implementation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee6c3e7",
   "metadata": {},
   "source": [
    "i) Write a function named init_params that initializes the parameters for each layer of the\n",
    "neural network. The input layer (a[0]) should have 784 units, the second layer (a[1]) should have\n",
    "120 units, the third layer (a[2]) should have 45 units, and the output layer (a[3]) should have 10\n",
    "units. Initialize the weight matrices (W) with random values between 0 and 1, and the bias\n",
    "vectors (b) with random values between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "7603dc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(): \n",
    "    layer_sizes = [784, 120, 45, 10]\n",
    "    \n",
    "    params = {}\n",
    "    \n",
    "    for i in range(1, 4):\n",
    "        input_size = layer_sizes[i - 1]\n",
    "        output_size = layer_sizes[i]\n",
    "        \n",
    "        params['W' + str(i)] = np.random.rand(output_size, input_size)\n",
    "        params['b' + str(i)] = np.random.rand(output_size, 1)\n",
    "        \n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7099098b",
   "metadata": {},
   "source": [
    "ii)Activation Functions:\n",
    "a) Implement the ReLU activation function in a function called ReLU.\n",
    "b) Implement the Softmax activation function in a function called Softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "15844605",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(Z):\n",
    "    A = np.maximum(Z, 0)\n",
    "    return A\n",
    "\n",
    "def Softmax(Z):\n",
    "    Z_scaled = Z / 100000\n",
    "    exp_Z = np.exp(Z_scaled)\n",
    "    sum_exp_Z = np.sum(exp_Z)\n",
    "    A = exp_Z / sum_exp_Z\n",
    "    return A * 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad3e5e4",
   "metadata": {},
   "source": [
    "iii)Forward Propagation:\n",
    "a) Create a function named forward_propagation that takes the weights (W) and\n",
    "biases (b) as input and performs forward propagation through the neural network.\n",
    "b) Implement the necessary calculations to compute the intermediate values (Z) and\n",
    "activations (A) for each layer.\n",
    "c) Apply the ReLU activation function to the intermediate values for the hidden\n",
    "layers (a[1] and a[2]).\n",
    "d) Apply the Softmax activation function to the intermediate values for the output\n",
    "layer (a[3]).\n",
    "e) Return the intermediate values (Z) and activations (A) for each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "01a397d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(params, X):\n",
    "    W1 = params['W1']\n",
    "    b1 = params['b1']\n",
    "    W2 = params['W2']\n",
    "    b2 = params['b2']\n",
    "    W3 = params['W3']\n",
    "    b3 = params['b3']\n",
    "    \n",
    "    Z1 = np.dot(W1, X.T) + b1\n",
    "    A1 = ReLU(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = ReLU(Z2)\n",
    "    Z3 = np.dot(W3, A2) + b3\n",
    "    A3 = Softmax(Z3)\n",
    "    \n",
    "    return Z1, A1, Z2, A2, Z3, A3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82443d5",
   "metadata": {},
   "source": [
    "iv) One-Hot Encoding:\n",
    "a) Implement a function called one_hot that converts an input array Y into its one-hot\n",
    "encoded representation.\n",
    "b) Use numpy arrays to assign a binary vector to each element in Y, setting the\n",
    "index corresponding to the element's value to 1 and all others to 0.\n",
    "c) Transpose the resulting array, where columns represent elements in Y and rows\n",
    "represent different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "9aedb4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(Y):\n",
    "    num_classes = 10\n",
    "    num_elements = Y.shape[0]\n",
    "    encoding = np.zeros(num_elements, num_classes)\n",
    "    for i in range(num_elements):\n",
    "        encoding[i, Y[i]] = 1\n",
    "    encoding = encoding.T\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643857e2",
   "metadata": {},
   "source": [
    "## Part 2: Backward Propagation and Model Training:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89215420",
   "metadata": {},
   "source": [
    "i)Backward Propagation:\n",
    "a) Write a function named backward_propagation that performs backward\n",
    "propagation to calculate the gradients of the parameters.\n",
    "b) Use the provided arguments Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X, and Y\n",
    "to calculate the gradients.\n",
    "c) Apply the appropriate activation functions' derivatives in the backpropagation\n",
    "process.\n",
    "d) Return the gradients dW1, db1, dW2, db2, dW3, and db3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "3dcbf6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU_derivative(Z):\n",
    "    return np.where(Z > 0, 1, 0)\n",
    "\n",
    "def backward_propagation(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X, Y):\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    dA3 = A3 - Y\n",
    "    dZ3 = dA3\n",
    "    dW3 = (1/m) * (np.dot(dZ3, A2.T))\n",
    "    db3 = (1/m) * (np.sum(dZ3, axis=1, keepdims=True))\n",
    "    \n",
    "    dA2 = np.dot(W3.T, dZ3)\n",
    "    dZ2 = dA2 * ReLU_derivative(Z2)\n",
    "    dW2 = (1/m) * (np.dot(dZ2, A1.T))\n",
    "    db2 = (1/m) * (np.sum(dZ2, axis=1, keepdims=True))\n",
    "    \n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    dZ1 = dA1 * ReLU_derivative(Z1)\n",
    "    dW1 = (1/m) * (np.dot(dZ1, X))\n",
    "    db1 = (1/m) * (np.sum(dZ1, axis=1, keepdims=True))\n",
    "    \n",
    "    return dW1, db1, dW2, db2, dW3, db3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58b5239",
   "metadata": {},
   "source": [
    "ii)Update Parameters:\n",
    "a) Implement a function named update_params that updates the parameters\n",
    "of the neural network using gradient descent.\n",
    "b) Use the provided arguments W1, b1, W2, b2, dW1, db1, dW2, db2, dW3, db3,\n",
    "and alpha (learning rate).\n",
    "c) Update the parameters using the gradients and the learning rate.\n",
    "d) Return the updated parameters W1, b1, W2, b2, W3, and b3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "0a7d31b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, alpha):\n",
    "    W1 = W1 - alpha * dW1\n",
    "    b1 = b1 - alpha * db1\n",
    "    \n",
    "    W2 = W2 - alpha * dW2\n",
    "    b2 = b2 - alpha * db2\n",
    "    \n",
    "    W3 = W3 - alpha * dW3\n",
    "    b3 = b3 - alpha * db3\n",
    "    \n",
    "    return W1, b1, W2, b2, W3, b3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1900f864",
   "metadata": {},
   "source": [
    "iii)Get Prediction and Accuracy:\n",
    "a) Create a function named get_prediction that takes the output activations\n",
    "(A3) as input and returns the predicted labels.\n",
    "b) Use numpy's argmax function to find the index of the highest value in each\n",
    "column of A3.\n",
    "c) Implement a function named get_accuracy that takes the predicted labels and\n",
    "true labels (Y) as input and calculates the accuracy.\n",
    "d) Print the predicted labels and true labels in the required format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "d78af3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(A3):\n",
    "    prediction = np.argmax(A3, axis=0)\n",
    "    return prediction\n",
    "\n",
    "def get_accuracy(prediction, Y):\n",
    "    print(prediction, Y)\n",
    "    condition = prediction==Y\n",
    "    accuracy = np.mean(condition)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cbea97",
   "metadata": {},
   "source": [
    "iv)Gradient Descent:\n",
    "a) Write a function named gradient_descent that performs the training of\n",
    "the neural network using gradient descent.\n",
    "b) Use the provided arguments X_train, Y_train, (alpha=0.1), and\n",
    "(num_iterations=1000).\n",
    "c) Perform the iterations of gradient descent, updating the parameters and tracking\n",
    "the accuracy every 10th iteration.\n",
    "d) Print the output layer prediction and accuracy during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "4156e43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X_train, Y_train, alpha=0.1, num_iterations=1000):\n",
    "\n",
    "    params = init_params()\n",
    "    W1 = params['W1']\n",
    "    b1 = params['b1']\n",
    "    W2 = params['W2']\n",
    "    b2 = params['b2']\n",
    "    W3 = params['W3']\n",
    "    b3 = params['b3']\n",
    "\n",
    "    for iteration in range(num_iterations):\n",
    "        Z1, A1, Z2, A2, Z3, A3 = forward_propagation(params, X_train)\n",
    "\n",
    "        dW1, db1, dW2, db2, dW3, db3 = backward_propagation(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X_train, Y_train)\n",
    "\n",
    "        W1, b1, W2, b2, W3, b3 = update_params(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, alpha)\n",
    "        \n",
    "        if iteration % 10 == 0:\n",
    "            predictions = get_prediction(A3)\n",
    "            accuracy = get_accuracy(predictions, Y_train)\n",
    "            print(\"Iteration:\", iteration)\n",
    "            print(\"Accuracy:\", accuracy)\n",
    "            print(\"\")\n",
    "\n",
    "    print(\"Final Output Layer Prediction:\")\n",
    "    print(predictions)\n",
    "    print(\"Final Accuracy:\", accuracy) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ff577f",
   "metadata": {},
   "source": [
    "## Part 3: Model Evaluation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f89036",
   "metadata": {},
   "source": [
    "i)Making Predictions:\n",
    "a) Implement a function named make_predictions that takes the inputs (X)\n",
    "and trained parameters (W1, b1, W2, b2) as input.\n",
    "b) Use the forward propagation function to obtain the predictions from the trained\n",
    "model.\n",
    "c) Return the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "5f7d9d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(X, W1, b1, W2, b2):\n",
    "    Z1, A1, Z2, A2, Z3, A3 = forward_propagation(X, W1, b1, W2, b2)\n",
    "    prediction = np.argmax(A3, axis=0)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea464d8",
   "metadata": {},
   "source": [
    "ii)Testing Predictions:\n",
    "a) Write a function named test_prediction that tests the model's predictions\n",
    "on a specific index of the training data.\n",
    "b) Use the provided arguments index, W1, b1, W2, and b2.\n",
    "c) Obtain the prediction and true label for the specified index.\n",
    "d) Visualize the image data using Matplotlib and print the prediction\n",
    "and true label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "24468a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_prediction(index, W1, b1, W2, b2):\n",
    "    x = X_train[index]\n",
    "    y_true = Y_train[index]\n",
    "    Z1, A1, Z2, A2, Z3, A3 = forward_propagation(x, W1, b1, W2, b2)\n",
    "    prediction = get_prediction(A3)\n",
    "\n",
    "    image = x.reshape(28, 28)\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.axis('off')\n",
    "\n",
    "    print(\"Prediction:\", prediction)\n",
    "    print(\"True Label:\", y_true)\n",
    "\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
